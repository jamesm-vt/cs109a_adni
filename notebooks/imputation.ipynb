{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Imputation\n",
    "\n",
    "The purpose of this notebook is to create design matrices by exploring different thresholds of missingness and executing different strategies for categorical and quantitative imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ADNI_utilities as utils\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RationalQuadratic\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "### Stacking libraries require mlxtend\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "from mlxtend.regressor import StackingCVRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This notebook uses [StackingCVClassifer][1] and [StackingCVRegressor][2] from the `mlxtend` library. Use the following commands to install `mlxtend` and to update other libs to required (latest) versions.*\n",
    "\n",
    "`\n",
    "conda config --add channels conda-forge\n",
    "conda install -y mlxtend\n",
    "conda update -y pandas\n",
    "conda update -y numpy\n",
    "conda update -y scikit-learn --no-channel-priority\n",
    "`\n",
    "\n",
    "[1]: http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/\n",
    "[2]: http://rasbt.github.io/mlxtend/user_guide/regressor/StackingRegressor/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data\n",
    "\n",
    "At this point, we have cleaned the data by removing duplicate/highly correlated or informationless features. We also have filled in missing values that could be found elsewhere within the collected study data. To deal with the remaining missingness, we will impute values. First we will define the necessary functions to do the following:\n",
    "* Drop columns due to missingness threshold\n",
    "* Combine ADNI Merged data with the cleaned and curated per-patient data\n",
    "* Normalize the data\n",
    "* Define the classifiers/regressors along with the ranges of parameters to be tuned\n",
    "* Identify categorical and quantitative features with missing values\n",
    "* Iterate over missing features, fit, predict, update, repeat\n",
    "* Save the intermediate and final results of imputataion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to strike a balance between keeping as much data as possible and reducing noise introduced by imputing columns that are nearly completely void of information. We will explore the relationship between these by imputing with different thresholds for missing data. The function below takes a threshold argument for missing values and drops any features from the dataset that exceed that threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_missing_cols(data, threshold):\n",
    "    \"\"\"Drops cols from data that have missing values above the threshold.\n",
    "    \n",
    "    # Arguments:\n",
    "        data: The data to drop missing cols from\n",
    "        threshold: float 0:1, the max allowable percentage of missing values\n",
    "        \n",
    "    # Returns\n",
    "        The data with missing cols dropped.\n",
    "    \"\"\"\n",
    "    cols = data.columns.values\n",
    "\n",
    "    # Drop columns that don't have at least 1 - threshold non-NA values\n",
    "    data = data.dropna(axis='columns', thresh=np.floor(data.shape[0] * (1 - threshold)))\n",
    "\n",
    "    print(f'Dropped columns due to missingness at threshold {threshold}:\\n', set(cols) ^ set(data.columns.values))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform imputation on datasets with different thresholds and compare the results. We also need to set the correct `dtype` for categorical features. Depending on the study phase, some categorical features use text values like `AD`, `MCI`, `MALE`, `FEMALE`, etc., and some use numerical codes. The per-patient data have been cleaned and standardized to use numeric values. We will read in a file that identifies the correct dtypes per column.\n",
    "\n",
    "Also, we have decided to normalize the data before imputing the missing values. We reason that imputing on the data before normalization may cover up or dilute any bias present in the data. The function below prepares the dataset by merging ADNI Merged and per-patient data, drops columns that exceed the missingness threshold, normalizes the data, and generates lists of categorical and quantitative features that will be required in the imputation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_data(missing_thresh):\n",
    "    \"\"\"Gets the raw combined (ADNI_Merge and Per_Patient) datasets. The\n",
    "    categorical features will have a dtype of 'int'. \n",
    "    \n",
    "    # Returns:\n",
    "        Returns the combined (ADNI_Merge and Per_Patient) datasets and lists\n",
    "        of the categorical and non-categorical features.\n",
    "    \"\"\"\n",
    "    # Read in the combined (Merged and Per Patient) dataset\n",
    "    pat_comb = pd.read_csv('../data/Per_Patient/pat_merge_b4_impute.csv', index_col='RID')\n",
    "    \n",
    "    # Drop cols based on missing threshold\n",
    "    pat_comb = drop_missing_cols(pat_comb, missing_thresh)\n",
    "\n",
    "    # Import the dtypes. Categorical variables are represented as int64 as opposed to floats.\n",
    "    dtypes = pd.read_csv('../data/Per_Patient/patient_firstidx_dtypes.csv', index_col='feature_name')\n",
    "\n",
    "    # Categoricals from baseline\n",
    "    categoricals = ['PTETHCAT', 'PTGENDER', 'PTRACCAT', 'PTMARRY', 'FSVERSION', 'APOE4', 'DX_bl']\n",
    "    for cat in categoricals:\n",
    "        if cat not in pat_comb.columns:\n",
    "            categoricals.pop(categoricals.index(cat))\n",
    "\n",
    "    # Collect categorical columns from pat_data based on dtype=int64\n",
    "    for i in dtypes.index:\n",
    "        # RID is the index so skip it\n",
    "        if i == 'RID':\n",
    "            continue\n",
    "        # If dtype is int, the it's categorical\n",
    "        if 'int' in dtypes.loc[i].data_type and i in pat_comb.columns:\n",
    "            categoricals.append(i)\n",
    "\n",
    "    # Remove any dupes\n",
    "    categoricals = list(set(categoricals))\n",
    "    non_cat = list(set(pat_comb.columns) ^ set(categoricals))\n",
    "    return pat_comb, categoricals, non_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking - Meta Ensemble Method\n",
    "\n",
    "Next we'll use a combination of stacking and grid search to see the effects of missingness and different models on imputation and predictive models. We'll start by defining the classifiers and regressors that we want to evaluate for imputing the missing data. Missing categorical values will be imputed first. During the model based imputation, we'll temporarily fill in missing values from quantitative features using mean based imputation.\n",
    "\n",
    "Because we have so much missing data in our combined dataset, we decided to use the stacking ensemble method for imputation to try to get the most accurate predictions possible during imputation. In stacking, the predictions of each base estimator are used as features for the *meta-estimator*. This allows the meta-estimator to appropriately weight the base models based on where they perform well or poorly. By selecting different classifiers/regressors and different parameters, we reason that we may get better predictions than any single model may provide independently. \n",
    "\n",
    "The function below creates a `StackingCVClassifier` that uses `SVC`, `KNN`, and `AdaBoostClassifier` with a `DecisionTreeClassifier` base estimator as the classifiers. The `SCV` classifier is wrapped in a `BaggingClassifier` which is itself wrapped in a `OneVsRestClassifier`. `LogisticRegression` was selected as the *meta-classifier*. This function returns the `StackingCVClassifer` along with a parameter grid for the base classifiers. These will later be wrapped in a `GridSearchCV` object to do cross validation using the different values of each estimator's tuning parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_clf_stack():\n",
    "    \"\"\"Convenience function to create a stacking meta classifier.\n",
    "    \n",
    "    # Returns:\n",
    "        A Stacking CV Classifier and a parameter grid.\n",
    "    \"\"\"\n",
    "    # Define classification estimators and their param grids\n",
    "    cl_params = {}\n",
    "\n",
    "    # SVC\n",
    "    svc = SVC(gamma='scale', decision_function_shape='ovr', probability=True, class_weight='balanced')\n",
    "    bclf = BaggingClassifier(svc, max_samples=.5, max_features=.5, n_estimators=10, n_jobs=-1)\n",
    "    clf1 = OneVsRestClassifier(bclf, n_jobs=-1)\n",
    "    cl_params['onevsrestclassifier__estimator__base_estimator__kernel'] = ['linear', 'rbf']\n",
    "    cl_params['onevsrestclassifier__estimator__base_estimator__C'] = 10. ** np.arange(-2, 2)\n",
    "\n",
    "    # AdaBoost with DecisionTree classifier\n",
    "    dtc = DecisionTreeClassifier(max_features=\"auto\", class_weight=\"balanced\", max_depth=None)\n",
    "    clf2 = AdaBoostClassifier(base_estimator=dtc)\n",
    "    cl_params['adaboostclassifier__base_estimator__criterion'] = [\"gini\"]\n",
    "    cl_params['adaboostclassifier__n_estimators'] = [100]\n",
    "\n",
    "    # KNN\n",
    "    clf3 = KNeighborsClassifier(n_jobs=-1)\n",
    "    cl_params['kneighborsclassifier__n_neighbors'] = [2, 5, 10, 100]\n",
    "\n",
    "    # Meta-classifier\n",
    "    meta_clf = LogisticRegression(solver='lbfgs', n_jobs=-1, multi_class='auto')\n",
    "\n",
    "    # Stacking-Classifier\n",
    "    sclf = StackingCVClassifier(classifiers=[clf1, clf2, clf3],\n",
    "                                meta_classifier=meta_clf)\n",
    "    \n",
    "    return sclf, cl_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a similar approach to impute missing values for quantitative features. For this, we wrote a function that creates a `StackingCVRegressor` that uses `linear_model.Lasso`, `linear_model.Ridge`, and `SVR` as the base regressors. `RandomForestRegressor` was chosen as the *meta-regressor*. This function returns the `StackingCVRegressor` and a parameter grid for the base regressors. Similar to the classification imputation, the `StackingCVRegressor`will later be wrapped in a `GridSearchCV` object to do cross validation using each estimator's tuning parameters provided in the parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_reg_stack():\n",
    "    \"\"\"Convenience function to create a stacking meta regressor.\n",
    "    \n",
    "    # Returns:\n",
    "        A Stacking CV Regressor and a parameter grid.\n",
    "    \"\"\"\n",
    "    # Define regression estimators and their param grids\n",
    "    reg_params = {}\n",
    "\n",
    "    # Lasso estimator\n",
    "    reg1 = linear_model.Lasso(selection='random', max_iter=3000)\n",
    "    reg_params['lasso__alpha'] = [.001, .01, .1]\n",
    "    \n",
    "    # Ridge estimator\n",
    "    reg2 = linear_model.Ridge(max_iter=3000)\n",
    "    reg_params['ridge__alpha'] = [.001, .01, .1]\n",
    "    \n",
    "    reg3 = SVR(kernel='rbf', gamma='auto')\n",
    "    reg_params['svr__C'] = [.01, 1, 10, 100]\n",
    "    \n",
    "    # RandomForest estimator\n",
    "    meta_reg = RandomForestRegressor(n_estimators=100, n_jobs=-1, oob_score=True)\n",
    "    reg_params['meta-randomforestregressor__min_samples_leaf'] = [3, 10, 50, 100]\n",
    "\n",
    "\n",
    "    reg_stack = StackingCVRegressor(regressors=(reg1, reg2, reg3),\n",
    "                            meta_regressor=meta_reg, \n",
    "                            use_features_in_secondary=True)\n",
    "    \n",
    "    return reg_stack, reg_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After imputation, we will save the results. Because we are doing KFold validation using many different estimators and different levels of missingness on all categorical and quantitative columns, the fitting/prediction process will run thousands of times. Because of this complexity, we catch and store any exceptions thrown during imputation so we can handle them separately. The function below stores the imputed data, scores, and any errors that occur during imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_imputation_data(data, imputed_cols, scores, errors, missing_thresh, model_name, tmp):\n",
    "    \"\"\"Saves the results of imputation. The imputed data is saved to disk with a name indicating\n",
    "    the threshold of missingness. Scores for imputed columns as well as an columns that were not\n",
    "    imputed due to errors are also stored to disk.\n",
    "    \n",
    "    # Arguments\n",
    "        data: Data with imputed values\n",
    "        imputed_cols: Columns that were successfully imputed.\n",
    "        scores: The score of the best estimator for each column\n",
    "        errors: List of columns that were not imputed due to exceptions thrown during\n",
    "            the imputation.\n",
    "        model_name: Name of the model (regressor or classifier)\n",
    "        tmp: Boolean flag to indicate whether this is temporary (intermediate) result.\n",
    "    \"\"\"\n",
    "    if tmp:\n",
    "        prefix = 'tmp_'\n",
    "    else:\n",
    "        prefix = ''\n",
    "        \n",
    "    imputed_data_file = f'../data/Imputed/{prefix}data_modeled_upto_{int(missing_thresh * 100)}pct_missing.csv'\n",
    "    data.to_csv(imputed_data_file)\n",
    "    \n",
    "    # Save models and scores\n",
    "    models, m_score = list(map(list, zip(*scores)))\n",
    "    \n",
    "    def name_model(model, col):\n",
    "        nm = model.__repr__().split('(')[0]\n",
    "        return f\"{col}_{missing_thresh}_{nm}\"\n",
    "    \n",
    "    model_names = [name_model(model, col) for model, col in zip(models, imputed_cols)]\n",
    "\n",
    "    scores_pd = pd.DataFrame({'Score': m_score, 'Model': model_names}, index=imputed_cols)\n",
    "    scores_pd.index.name = 'Feature'\n",
    "    \n",
    "    if not tmp:\n",
    "        tmp_file = f'../data/Imputed/tmp_scores_modeled_upto_{int(missing_thresh * 100)}pct_missing.csv'\n",
    "        tmp_scores = pd.read_csv(tmp_file, index_col='Feature')\n",
    "        scores_pd = pd.concat([scores_pd, tmp_scores], sort=True)\n",
    "    \n",
    "    imputed_scores_file = f'../data/Imputed/{prefix}scores_modeled_upto_{int(missing_thresh * 100)}pct_missing.csv'\n",
    "    scores_pd.to_csv(imputed_scores_file)\n",
    "    \n",
    "    # Save errors\n",
    "    if errors is not None:\n",
    "        err_file = f'../data/Imputed/errors/impute_errors_upto_{int(missing_thresh * 100)}pct_{model_name}.csv'\n",
    "        try:\n",
    "            pd.Series(errors).to_csv(err_file)\n",
    "        except:\n",
    "            print(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the actual imputation, we will iterate over a few different thresholds of missingness. We chose `[1, .5, .3]` based on the distribution of missingness described in the ADNI Merged EDA section. These numbers describe our tolerance for missing values. The number $1$, for example, means we will keep columns with up to $100\\%$ missing values. *Technically we need at least 1 non-null value.* The other values, $.5$ and .$3$, mean we will accept columns with up to $50\\%$ and $30\\%$ missing values respectively.\n",
    "\n",
    "#### *NOTE: The following code will take several hours to complete*\n",
    "With this approach, we end up performing imputation nearly 460 times to account for every categorical and quantitative feature in each missingness threshold. Because we have many different estimators in `StackingCVRegressor` and `StackingCVClassifier` ***and*** we're doing grid-based cross-validation in `utils.impute_values_classification(...)` and `utils.impute_values_regression(...)`, the following code ends up fitting, predicting, and scoring tens of thousands of times.\n",
    "\n",
    "#### Leveraging AWS cloud compute resources\n",
    "To get the imputation to run in a reasonable amount of time, we used `n_jobs=-1` wherever possible. We also launched an [AWS c5.9xlarge EC2][1] compute intance with $36$ vCPUs to leverage parallelism where possible. We used an [AWS Deep Learning AMI][2] since most of the required libraries came pre-installed. To complete setting up the instance, we made a clone our git repo and installed `mlxtend`. Once the instance was launched and configured, we only needed to [open a tunnel][3] between our local machine and the remote instance and then start the jupyter notebook server on the EC2 instance. Even when utilizing the robust c5.9xlarge compute instance with 36 vCPUs, it still took approximately 12 hours for this code to complete execution.\n",
    "\n",
    "[1]: https://aws.amazon.com/blogs/aws/now-available-compute-intensive-c5-instances-for-amazon-ec2/\n",
    "[2]: https://aws.amazon.com/blogs/machine-learning/get-started-with-deep-learning-using-the-aws-deep-learning-ami/\n",
    "[3]: https://docs.aws.amazon.com/dlami/latest/devguide/setup-jupyter-configure-client.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns due to missingness at threshold 1:\n",
      " {}\n",
      "\n",
      "Imputing feature 1 of 88: BCELMOOD\n",
      "Imputing feature 2 of 88: DXCOMB\n",
      "Imputing feature 3 of 88: DX_FINAL\n",
      "\n",
      "[ Output truncated...]\n"
     ]
    }
   ],
   "source": [
    "thresholds = [1, .5, .3]\n",
    "\n",
    "for missing_thresh in thresholds:\n",
    "    # Read in data and identify categorical and non-categorical features\n",
    "    pat_comb, categoricals, non_cat = get_combined_data(missing_thresh)\n",
    "    \n",
    "    # Scale all columns that are non-categorical\n",
    "    pat_comb = utils.scale_cols(data=pat_comb, cols=non_cat, scaler=MinMaxScaler())\n",
    "    \n",
    "    # Create stacking classifier\n",
    "    sclf, cl_params = make_clf_stack()\n",
    "    \n",
    "    # Impute categorical values\n",
    "    pat_comb, imputed_cols, scores, errors = utils.impute_values_classification(pat_comb,\n",
    "                                                      cols=categoricals,\n",
    "                                                      estimator=sclf,\n",
    "                                                      param_grid=cl_params)\n",
    "    \n",
    "    # Save intermediate data\n",
    "    save_imputation_data(pat_comb, imputed_cols, scores,\n",
    "                         errors, missing_thresh,\n",
    "                         'StackingCVClassifier', tmp=True)\n",
    "    \n",
    "    # Create stacking regressor\n",
    "    reg_stack, reg_params = make_reg_stack()\n",
    "    \n",
    "    # Impute non-categorical values\n",
    "    pat_comb, imputed_cols, scores = utils.impute_values_regression(pat_comb,\n",
    "                                                      cols=non_cat,\n",
    "                                                      estimator=reg_stack,\n",
    "                                                      param_grid=reg_params)\n",
    "    \n",
    "    # Save final data\n",
    "    save_imputation_data(pat_comb, imputed_cols, scores,\n",
    "                         None, missing_thresh,\n",
    "                         'StackingCVRegressor', tmp=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with errors\n",
    "One result of our approach to imputation is that the imputation process failed for some categorical features. It turns out that when doing KFold cross validation, a stratified split cannot be done when the number of instances of a class category is less than the number of folds. We addressed this by catching those exceptions and storing the name of the column that was not imputed. After storing the column name, processing and imputation of other features continued. Finally, the list of features that were not successfully imputed were written to disk. \n",
    "\n",
    "Even though the number of errors were relatively small, we wanted to keep these data in our dataset. The following code reads those non-imputed features from disk and performs non-cv model-based imputation. Once the missing values are imputed, the results are added back in and the design matrices are ready for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 1:\n",
      "\n",
      "Imputing feature 1 of 21: BCWANDER\n",
      "Imputing feature 2 of 21: MOMDEM\n",
      "Imputing feature 3 of 21: FSVERSION\n",
      "Imputing feature 4 of 21: BCCHEST\n",
      "Imputing feature 5 of 21: NXABNORM\n",
      "Imputing feature 6 of 21: DXPARK\n",
      "Imputing feature 7 of 21: DXMPTR4\n",
      "Imputing feature 8 of 21: BCURNDIS\n",
      "Imputing feature 9 of 21: BCVOMIT\n",
      "Imputing feature 10 of 21: APGEN2\n",
      "Imputing feature 11 of 21: BCELMOOD\n",
      "Imputing feature 12 of 21: DXMPTR2\n",
      "Imputing feature 13 of 21: DXMPTR3\n",
      "Imputing feature 14 of 21: DXNODEP\n",
      "Imputing feature 15 of 21: NXHEEL\n",
      "Imputing feature 16 of 21: DXMPTR5\n",
      "Imputing feature 17 of 21: DXNORM\n",
      "Imputing feature 18 of 21: NXCONSCI\n",
      "Imputing feature 19 of 21: DXMPTR1\n",
      "Imputing feature 20 of 21: MH15DRUG\n",
      "Imputing feature 21 of 21: DX_CHANGE\n",
      "\n",
      "Threshold 0.5:\n",
      "\n",
      "Imputing feature 1 of 14: FSVERSION\n",
      "Imputing feature 2 of 14: NXABNORM\n",
      "Imputing feature 3 of 14: BCVOMIT\n",
      "Imputing feature 4 of 14: NXCONSCI\n",
      "Imputing feature 5 of 14: BCELMOOD\n",
      "Imputing feature 6 of 14: NXHEEL\n",
      "Imputing feature 7 of 14: APGEN2\n",
      "Imputing feature 8 of 14: DX_CHANGE\n",
      "Imputing feature 9 of 14: BCWANDER\n",
      "Imputing feature 10 of 14: MOMDEM\n",
      "Imputing feature 11 of 14: DXPARK\n",
      "Imputing feature 12 of 14: BCURNDIS\n",
      "Imputing feature 13 of 14: MH15DRUG\n",
      "Imputing feature 14 of 14: BCCHEST\n",
      "\n",
      "Threshold 0.3:\n",
      "\n",
      "Imputing feature 1 of 13: BCELMOOD\n",
      "Imputing feature 2 of 13: BCURNDIS\n",
      "Imputing feature 3 of 13: NXCONSCI\n",
      "Imputing feature 4 of 13: NXABNORM\n",
      "Imputing feature 5 of 13: DX_CHANGE\n",
      "Imputing feature 6 of 13: NXHEEL\n",
      "Imputing feature 7 of 13: BCCHEST\n",
      "Imputing feature 8 of 13: BCWANDER\n",
      "Imputing feature 9 of 13: BCVOMIT\n",
      "Imputing feature 10 of 13: FSVERSION\n",
      "Imputing feature 11 of 13: MOMDEM\n",
      "Imputing feature 12 of 13: APGEN2\n",
      "Imputing feature 13 of 13: MH15DRUG\n"
     ]
    }
   ],
   "source": [
    "thresholds = [1, .5, .3]\n",
    "for missing_thresh in thresholds:\n",
    "\n",
    "    print(\"Thres\")\n",
    "    # Read in data and identify categorical and non-categorical features\n",
    "    pre_imputed, categoricals, non_cat = get_combined_data(missing_thresh)\n",
    "    \n",
    "    # Scale all columns that are non-categorical\n",
    "    pre_imputed = utils.scale_cols(data=pre_imputed, cols=non_cat, scaler=MinMaxScaler())\n",
    "    \n",
    "    err_file = f'../data/Imputed/errors/impute_errors_upto_{int(missing_thresh * 100)}pct_StackingCVClassifier.csv'\n",
    "    dat_file = f'../data/Imputed/data_modeled_upto_{int(missing_thresh * 100)}pct_missing.csv'\n",
    "    \n",
    "    errs = pd.read_csv(err_file, header=None, index_col=0, names=['Column_Name'])['Column_Name'].values\n",
    "    data = pd.read_csv(dat_file, index_col='RID')\n",
    "    \n",
    "    data = pd.concat([data, pre_imputed[errs]], axis=1)\n",
    "    data, imputed, scores, errors = utils.impute_errors(data, list(errs))\n",
    "    \n",
    "    data.to_csv(dat_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand if our stacking/cv model-based imputation approach was worth the effort, we created mean/mode imputed design matrices at $.5$ and $.3$ thresholds of missingness for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns due to missingness at threshold 0.5:\n",
      " {'DADAD', 'UAT3', 'DXNORM', 'DXMPTR5', 'DXMPTR2', 'DXMPTR4', 'DXMPTR1', 'HMT96', 'MH14BALCH', 'BAT324', 'HMT93', 'MH15BDRUG', 'MH14AALCH', 'UAT2', 'AXT117', 'MH14CALCH', 'HMT95', 'CMT2', 'HMT98', 'HMT20', 'DXMPTR3', 'DXNODEP', 'CMT3', 'HMT70', 'MH16CSMOK', 'DIGITSCOR', 'HMT69', 'MH16ASMOK', 'HMT21', 'MH15ADRUG', 'MH16BSMOK'}\n",
      "\n",
      "Dropped columns due to missingness at threshold 0.3:\n",
      " {'EcogPtPlan', 'DADAD', 'UAT3', 'DXNORM', 'DXMPTR5', 'EcogPtMem', 'DXMPTR2', 'DXMPTR4', 'PTAU', 'DXDEP', 'DXMPTR1', 'ABETA', 'FDG', 'HMT96', 'MH14BALCH', 'BAT324', 'HMT93', 'EcogPtOrgan', 'MH15BDRUG', 'DXPARK', 'MH14AALCH', 'TAU', 'EcogPtTotal', 'UAT2', 'AXT117', 'MH14CALCH', 'EcogSPPlan', 'EcogPtVisspat', 'HMT95', 'CMT2', 'HMT98', 'EcogSPVisspat', 'HMT20', 'DXMPTR3', 'EcogSPTotal', 'EcogPtDivatt', 'EcogSPLang', 'EcogPtLang', 'DXNODEP', 'CMT3', 'HMT70', 'MH16CSMOK', 'DIGITSCOR', 'MOMAD', 'AV45', 'HMT69', 'MH16ASMOK', 'MOCA', 'HMT21', 'MH15ADRUG', 'EcogSPOrgan', 'MH16BSMOK', 'EcogSPMem', 'EcogSPDivatt'}\n"
     ]
    }
   ],
   "source": [
    "thresholds = [.5, .3]\n",
    "for missing_thresh in thresholds:\n",
    "\n",
    "    # Read in data and identify categorical and non-categorical features\n",
    "    pat_comb, categoricals, non_cat = get_combined_data(missing_thresh)\n",
    "\n",
    "    # Scale all columns that are non-categorical\n",
    "    pat_comb = utils.scale_cols(data=pat_comb, cols=non_cat, scaler=MinMaxScaler())\n",
    "\n",
    "    non_cat_cols = pat_comb[non_cat]\n",
    "    non_cat_cols = non_cat_cols.fillna(non_cat_cols.mean())\n",
    "\n",
    "\n",
    "    cat_cols = pat_comb[categoricals]\n",
    "    cat_cols = cat_cols.fillna(cat_cols.mode().iloc[0])\n",
    "\n",
    "    pat_comb.update(non_cat_cols)\n",
    "    pat_comb.update(cat_cols)\n",
    "\n",
    "    mean_mode_file = f'../data/Imputed/data_mean_upto_{int(missing_thresh * 100)}pct_missing.csv'\n",
    "    pat_comb.to_csv(mean_mode_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to evaluate the effectiveness of adding the additional per-patient data to the ADNI Merged dataset, we created mean/mode imputed design matrices from the ADNI Merged data at the $.5$ and $.3$ missingness threshold levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns due to missingness at threshold 0.5:\n",
      " {'DIGITSCOR'}\n",
      "\n",
      "Dropped columns due to missingness at threshold 0.3:\n",
      " {'FDG', 'DIGITSCOR', 'EcogPtPlan', 'EcogSPVisspat', 'AV45', 'EcogPtOrgan', 'MOCA', 'EcogPtMem', 'EcogPtDivatt', 'EcogSPTotal', 'EcogSPLang', 'EcogSPOrgan', 'EcogPtLang', 'TAU', 'EcogPtTotal', 'PTAU', 'ABETA', 'EcogSPPlan', 'EcogPtVisspat', 'EcogSPMem', 'EcogSPDivatt'}\n"
     ]
    }
   ],
   "source": [
    "thresholds = [.5, .3]\n",
    "for missing_thresh in thresholds:\n",
    "\n",
    "    # Drop cols based on missing threshold\n",
    "    bl_imputed = drop_missing_cols(baseline, missing_thresh)\n",
    "\n",
    "    categoricals = ['PTETHCAT', 'PTGENDER', 'PTRACCAT', 'PTMARRY', 'FSVERSION', 'APOE4', 'DX_bl']\n",
    "    non_cat = list(set(bl_imputed.columns) ^ set(categoricals))\n",
    "    \n",
    "    # Scale all columns that are non-categorical\n",
    "    bl_imputed = utils.scale_cols(data=bl_imputed, cols=non_cat, scaler=MinMaxScaler())\n",
    "\n",
    "    # Impute using mean\n",
    "    non_cat_cols = bl_imputed[non_cat]\n",
    "    non_cat_cols = non_cat_cols.fillna(non_cat_cols.mean())\n",
    "\n",
    "    # Impute using mode\n",
    "    cat_cols = bl_imputed[categoricals]\n",
    "    cat_cols = cat_cols.fillna(cat_cols.mode().iloc[0])\n",
    "\n",
    "    # Update baseline with imputed values\n",
    "    bl_imputed.update(non_cat_cols)\n",
    "    bl_imputed.update(cat_cols)\n",
    "\n",
    "    # Save the results to disk\n",
    "    mean_mode_file = f'../data/Imputed/baseline_mean_upto_{int(missing_thresh * 100)}pct_missing.csv'\n",
    "    bl_imputed.to_csv(mean_mode_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
