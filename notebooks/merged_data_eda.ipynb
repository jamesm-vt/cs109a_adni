{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alzheimer's Disease Neuroimaging Initiative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "with open(\"project.css\") as css:\n",
    "    styles = css.read()\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADNI\n",
    "\n",
    "The overarching goal of the Alzheimer's Disease Neuroimaging Initiative (ADNI) is to identify biomarkers of Alzheimer’s disease. Specifically the study aims at identifying those biomarkers that can be used in the earliest (pre-dementia) prediction of Alzheimer's Disease (AD). The study began in 2004, a time when biomarkers for Alzheimer’s disease which could be used for diagnostics in pre-dementia individuals were virtually unknown. There are four categories of biomarkers in the scope of the initiative: (clinical, imaging, genetic, and biochemical).\n",
    "\n",
    "There have been ADNI 4 study phases to date with the following goals:\n",
    "\n",
    "<!-- Begin ADNI Phase table -->\n",
    "\n",
    "| Study Phase | Goal | Dates | Cohort |\n",
    "|:---: |:--- |:--- | --- |\n",
    "| ADNI 1 | Develop biomarkers as outcome measures for clinical trials | 2004-2009 | 200 elderly controls<br>400 MCI<br>200 AD |\n",
    "| ADNI GO | Examine biomarkers in earlier stages of disease | 2009-2011 | Existing ADNI-1 +<br>200 early MCI |\n",
    "| ADNI 2 | Develop biomarkers as predictors of cognitive decline, and as outcome measures | 2011-2016 | Existing ADNI-1 and ADNI-GO +<br>150 elderly controls<br>100 early MCI<br>150 late MCI<br>150 AD |\n",
    "| ADNI 3 | Study the use of tau PET and functional imaging techniques in clinical trials | 2016 - present | Existing ADNI-1, ADNI-GO, ADNI-2 +<br>133 elderly controls<br>151 MCI<br>87 AD |\n",
    "\n",
    "<!-- End ADNI phase table -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADNI Data\n",
    "Before looking at a single observation or feature, there is a lot of information we can glean from reviewing ADNI metadata. There are over 250 datasets in the ADNI data inventory spanning the 4 study phases (ADNI 1, GO, 2, 3) - and this number does not include the archives. These studies are longitudinal. ADNI-1 started in 2004 and ADNI-3 continues today. Although there is potentially a wealth of information, insights, and predictive power in these data, their data collection methods and longitudinal nature present many challenges.\n",
    "\n",
    "One challenge is that all biometrics within the scope of the study are not collected across all study phases. Also, within each phase, not all participants had all measurements taken. For example, in ADNI-1, $100\\%$ of the cohort had a 1.5 Tesla (1.5T) MRI, $50\\%$ had a PET scan. Of the $50\\%$ that didn't have a PET scan, $25\\%$ had a 3T MRI. Finally, only $20\\%$ of the ADNI-1 cohort had a lumbar puncture (L.P.) to collect cerebral spinal fluid (CSF).\n",
    "\n",
    "Other data challenges are related to the longitudinal nature of the studies across the different phases. In each successive phase of the study, participants were rolled over from previous phases while new participants were also added - *(cohort details can be seen in the table above)*. However, existing participants in the study must provide their consent to be included in each subsequent phase. Furthermore, an obvious, but nonetheless real, complication with this population is that a participant could be removed from the study at any time due to significant deterioration in health or death. \n",
    "\n",
    "The result is that each phase of the study produces a richer set of longitudinal data than the previous study because of the rollover participants. The downside of this design is the inherent introduction of missingness into the data due to the recently joined participants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An initial look at the data.\n",
    "Given the breadth of available data and the challenges mentioned above, deciding what data to use to start EDA is a non-trivial task. Fortunately, there is a combined dataset available consisting of key ADNI tables merged into a single table based on the patient identifier or `RID`. As is common with most ADNI datasets, each observation represents a single visit for a participant. This means that a single participant (`RID`) may appear multiple times in the dataset. The number of occurrences will generally depend on what phase the participant entered the study.\n",
    "\n",
    "Let's take an initial look at the merged dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ADNI_utilities as utils\n",
    "\n",
    "from pandas.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adni_merge = pd.read_csv('../data/ADNIMERGE.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape of the dataset is {}.\".format(adni_merge.shape))\n",
    "print(\"There are {} unique participants in the dataset.\"\n",
    "      .format(len(adni_merge.RID.unique())))\n",
    "print(\"There is an average of {:.2f} rows in the data per participant.\"\n",
    "      .format(len(adni_merge)/len(adni_merge.RID.unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the dataset contains $2081$ unique study participants spanning $13632$ visits. The data is longitudinal based on participant visits spaced roughly six months apart. The `VISCODE` feature represents the visit in which the measurements and evaluations were captured. The initial evaluation measurements are identified by `VISCODE` = `'bl'`, which stands for baseline. Below are the unique `VISCODE` values in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adni_merge.sort_values(by='Month')['VISCODE'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the number of study participants per `VISCODE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M represents months since the last visit (0 = baseline/initial visit)\n",
    "adni_by_month = adni_merge.groupby(by='M').count()\n",
    "particpants = adni_by_month['RID']\n",
    "visits = adni_merge.sort_values(by='M')['VISCODE'].unique()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14,8))\n",
    "\n",
    "ax.set_title('Number of participants per visit', size=17)\n",
    "ax.set_xticks(range(0, 26, 2))\n",
    "ax.set_xlabel('Visit Code (indicates months since inital baseline visit)', size=14)\n",
    "ax.set_ylabel('Participants', size=14)\n",
    "ax.bar(visits, particpants)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the design of the study as discussed above, we expect there to be a lot of missing data in this data set. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missing data\n",
    "missing_data = utils.calculate_missing_data(adni_merge)\n",
    "\n",
    "# Look at the top 10 columns in terms of missing values\n",
    "missing_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've noticed that the numbers of non-null values for `PTAU`, `ABETA`, and `TAU` are suspiciously close to the number of unique participants. The fact that these features all have almost the exact same number of missing values could be an artifact of how and when these data were collected. Perhaps these were collected on the initial baseline `bl` visit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_proteins = missing_data.loc[['TAU', 'PTAU', 'ABETA']]\n",
    "missing_proteins['Num Values Present'] = len(adni_merge) - missing_proteins['Num Missing']\n",
    "missing_proteins['Num Participants'] = len(adni_merge.RID.unique())\n",
    "missing_proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many options to deal with the variable number of visits in the merged data set. Instead of vertically stacking the visits as in the merged dataset, we could split on `VISCODE` and stack the data *horizontally* creating wide rows with many more features. However, this is essentially transposing the data and moving the missing values from deep columns to wide rows. Another option is to split the data into multiple subsets of data based on `VISCODE` and deal with them separately. As shown in the \"*Participants per visit*\" figure, every participant had at least a baseline visit. This subset should provide the most complete and uniform representation of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = adni_merge[adni_merge['VISCODE'] == 'bl'].copy()\n",
    "print('Shape of the baseline visit subset: ', baseline.shape)\n",
    "\n",
    "baseline.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examime the missing data from the baseline visit dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missing data\n",
    "missing_data = utils.calculate_missing_data(baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling in Missing Data\n",
    "Based on the study data, we know that not all biometrics are measured at every visit. Therefore we may be able to pull measures together from different visits to help fill in missing data. Of course since we're dealing with longitudinal data with visits month or years apart, we have to make sure that we only consider measures from visits where the diagnosis code is unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For every column in the baseline (VISCODE='bl') with missing values, look for\n",
    "# a value in subsequent visits with the constraint that the DX code must\n",
    "# not have changed. Take the first (earliest) biometric measure available.\n",
    "\n",
    "baseline.sort_values(by='RID')\n",
    "missing_cols = baseline.columns[baseline.isnull().any()]\n",
    "viscodes = list(adni_merge.sort_values(by='Month')['VISCODE'].unique())\n",
    "viscodes.pop(0) # Get rid of 'bl'\n",
    "\n",
    "missing_values = baseline.isnull().sum().sum()\n",
    "updated_values = 0\n",
    "\n",
    "print('Searching...')\n",
    "for col in missing_cols:\n",
    "    \n",
    "    for v in viscodes:\n",
    "        \n",
    "        # Get the RIDs with missing values in this colummn.\n",
    "        # Do this for each VISCODE since we are iteratively\n",
    "        # updating missing values.\n",
    "        rids = baseline[baseline[col].isnull()].RID\n",
    "        \n",
    "        # Create a DataFrame from adni_merge for the current\n",
    "        # VISCODE, RIDs, & where current col is not null.\n",
    "        df = adni_merge.loc[(adni_merge.RID.isin(rids))\n",
    "                            & (adni_merge.VISCODE == v)\n",
    "                            & (adni_merge[col].notnull()),\n",
    "                            baseline.columns] \n",
    "               \n",
    "        if df.empty: # if no matches, continue\n",
    "            continue\n",
    "            \n",
    "        df = df.copy()\n",
    "        df.sort_values(by='RID', inplace=True)\n",
    "            \n",
    "        # Find baseline participants who are also in the current VISCODE\n",
    "        bl = baseline[baseline.RID.isin(df.RID)].copy()\n",
    "        bl.sort_values(by='RID', inplace=True)\n",
    "        df.index = bl.index\n",
    "        \n",
    "        # Only keep those where the diagnosis is unchanged & col is not null\n",
    "        df = df[(df.DX == bl.DX) & (df[col].notnull())]\n",
    "\n",
    "        if df.empty:  # if DX codes don't match, continue\n",
    "            continue\n",
    "\n",
    "        # Update null values in the original baseline DF\n",
    "        baseline.loc[baseline.index.isin(df.index), col] = df[col]\n",
    "        updated_values += len(df)\n",
    "\n",
    "print(f'Updated {updated_values} of {missing_values} missing values.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missing data\n",
    "missing_data = utils.calculate_missing_data(baseline)\n",
    "\n",
    "# Look at the top 10 columns in terms of missing values\n",
    "missing_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly a lot of missing data remain and we will likely have to explore methods to impute these values. Before doing that however, we will explore the data a little closer to see if there are features that should be dropped due to high correlation, lack of information, or other reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`FLDSTRENG` and `FLDSTRENG_bl` are providing absolutely no information so we can drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = baseline.drop(labels=['FLDSTRENG', 'FLDSTRENG_bl'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be a lot of features with similar, if not identical, information such as `TAU`, `TAU_bl`, `AV45`, `AV45_bl`. Let's examine this pattern to see if these pairs are highly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a correlation matrix of xxx_bl vs xxx\n",
    "corr_df = baseline.corr()\n",
    "cols = baseline.columns\n",
    "col1, col2, corr = [], [], []\n",
    "\n",
    "# Specifically check the correlation of xxx_bl to xxx\n",
    "for col in cols:\n",
    "    if '_bl' in col.lower(): \n",
    "        drop_bl = col[0:-3]\n",
    "        if (drop_bl in cols):\n",
    "            if (col in corr_df.index and corr_df.loc[col][drop_bl] > .8):\n",
    "                col1.append(col)\n",
    "                col2.append(drop_bl)\n",
    "                corr.append(corr_df.loc[col][drop_bl])\n",
    "\n",
    "# Display the results                \n",
    "bl_corr_df = pd.DataFrame({\"Baseline column\": col1, 'Alternate column': col2, \"Correlation\": corr})\n",
    "bl_corr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These features indeed contain duplicate information so we can drop one of each pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = baseline.drop(labels=bl_corr_df['Baseline column'].values, axis=1)\n",
    "print(\"The new shape of the baseline subset is {}.\".format(baseline.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what features we have now\n",
    "baseline.columns.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have some columns that look very similar. These may contain non-numeric values such as strings or NaNs, or possibly they are really uncorrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to check for duplicates\n",
    "cols = ['ABETA', 'DX', 'EXAMDATE','FSVERSION',  'PTAU', 'TAU']\n",
    "\n",
    "bl_missing = []\n",
    "missing = []\n",
    "matching_vals = []\n",
    "\n",
    "for col in cols:\n",
    "    missing.append(baseline[col].isnull().sum())\n",
    "    bl_missing.append(baseline[col+'_bl'].isnull().sum())\n",
    "    match = (baseline[col] == baseline[col+'_bl']).sum()\n",
    "    matching_vals.append((match + min(missing[-1], bl_missing[-1]))/len(baseline) * 100)\n",
    "                 \n",
    "# Display the results                \n",
    "bl_dupes = pd.DataFrame({'Missing Values': missing,\n",
    "                            'Baseline Missing Values': bl_missing,\n",
    "                            'Percent Matching': matching_vals}, index=cols)\n",
    "bl_dupes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the pairs are nearly exact duplicates except `DX`|`DX_bl`, so we can drop one of the duplicate columns. The baseline version has slightly more missing data, so we'll drop that one. Then we'll take a look at `DX` vs.`DX_bl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of duplicate column names to drop\n",
    "dupe_cols = [col + '_bl' for col in cols]\n",
    "\n",
    "# Remove DX_bl until we investigate further\n",
    "del dupe_cols[dupe_cols.index('DX_bl')]\n",
    "\n",
    "# Drop the columns\n",
    "baseline = baseline.drop(labels=dupe_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how DX maps to DX_bl\n",
    "baseline.drop_duplicates('DX_bl')[['DX_bl', 'DX']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although similar, the diagnoses in `DX_bl` are more specific than those in `DX`. Having the more specific classifications may be useful so we'll keep them and then take another look at our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = baseline.drop(labels=['DX'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have some participants for which we have no diagnosis code, so these records will not be useful and can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_dx = len(baseline[baseline['DX_bl'].isnull()])\n",
    "baseline = baseline.dropna(axis=0, subset=['DX_bl'])\n",
    "print(f'Removed {missing_dx} participants with no diagnosis code.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PTID` is also duplicative. It is a combination of `RID` and `SITE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = baseline.drop(labels='PTID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The new shape of the baseline subset is {}.\".format(baseline.shape))\n",
    "utils.calculate_missing_data(baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the new dataset, it is clear that `PIB` will not be useful and can be removed. PIB or *PiB* stands for **Pi**ttsburgh Compound-**B** - a synthetic radiotracer developed for use in PET scans to visualize and measure A$\\beta$ deposits in the brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = baseline.drop(labels='PIB', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove other features that won't be helpful in predicting AD, such as `SITE`, `Month`, `EXAMDATE`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = baseline.drop(labels=['update_stamp', 'Years_bl', 'SITE', 'VISCODE', 'COLPROT', 'ORIGPROT',\n",
    "                                 'Month_bl', 'Month', 'M', 'EXAMDATE', 'IMAGEUID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what features we have now\n",
    "baseline.columns.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have cleaned the data by removing duplicate/highly correlated or informationless features and filling in missing values that can be found within the collected data. To deal with the remaining missingness, we will impute values. But first we have to make a decision about whether to normalize the data before or after imputation. We have decided to standardize the data before imputing missing values. We reason that imputing on the data before standardization may cover up or dilute any bias present in the data. Also, standardizing the data before imputation is preferred if we use modeling to impute missing data.\n",
    "\n",
    "We'll start by preparing the categorical features and making sure the `dtypes` are appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features:\n",
    "categoricals = ['PTETHCAT', 'PTGENDER', 'PTRACCAT', 'PTMARRY', 'FSVERSION', 'APOE4', 'DX_bl']\n",
    "baseline = pd.get_dummies(baseline, columns=categoricals, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some non-numeric data in `ABETA`, `TAU`, and `PTAU`, such as '>1300' or '<80'. We'll remove the `>` and `<` characters and change the dtype to float64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove < or > \n",
    "def remove_gt_lt(val):\n",
    "    if type(val) == str:\n",
    "        return float(val.replace('>', '').replace('<', ''))\n",
    "    else:\n",
    "        return val\n",
    "    \n",
    "    \n",
    "for col in ['ABETA', 'TAU', 'PTAU']:\n",
    "    values = baseline[col].values\n",
    "    baseline[col] = list(map(remove_gt_lt, values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline['PTEDUCAT'] = baseline.PTEDUCAT.astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are additional (potentially valuable) data that was not included in the Merged data set. These data have been cleaned and put in a format such that we can join them to the merged data set. We will do that before performing any imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_data = pd.read_csv('../data/Per_Patient/patient_firstidx_merge.csv', index_col='RID', na_values=-1)\n",
    "pat_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline.index = baseline['RID']\n",
    "baseline = baseline.drop(labels='RID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline = pd.merge(baseline, pat_data, key='RID')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
